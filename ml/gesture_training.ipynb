{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a1e6774",
   "metadata": {},
   "source": [
    "# ESP32 Air Mouse – Gesture Training Notebook\n",
    "\n",
    "This notebook helps you train a simple gesture classifier from raw IMU logs collected via the desktop app.\n",
    "\n",
    "What you’ll do:\n",
    "- Load CSV logs (columns: timestamp, ax, ay, az, gx, gy, gz, label)\n",
    "- Window the data (default: 1.0 s windows at ~100 Hz, 50% overlap)\n",
    "- Extract simple, robust features (stats + band energy)\n",
    "- Train and evaluate:\n",
    "  - Baseline: Random Forest\n",
    "  - Optional: Small TensorFlow dense classifier\n",
    "- Export: TFLite int8 model and a C array header for firmware\n",
    "\n",
    "Tips:\n",
    "- Keep gestures short (1–2 s) and clearly labeled.\n",
    "- Capture at least 50–100 windows per class.\n",
    "- Balance classes or use class weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "00db5418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running for the first time, you may need these. They’ll only install if missing.\n",
    "import sys, subprocess\n",
    "\n",
    "def ensure(pkg, import_name=None):\n",
    "    import importlib\n",
    "    try:\n",
    "        importlib.import_module(import_name or pkg)\n",
    "    except ImportError:\n",
    "        print(f\"Installing {pkg}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg])\n",
    "\n",
    "# Core\n",
    "ensure(\"numpy\")\n",
    "ensure(\"pandas\")\n",
    "ensure(\"scikit-learn\", \"sklearn\")\n",
    "ensure(\"scipy\")\n",
    "ensure(\"matplotlib\")\n",
    "# Optional deep learning (skip if you only want RandomForest)\n",
    "try:\n",
    "    import tensorflow as tf  # noqa: F401\n",
    "except Exception:\n",
    "    print(\"TensorFlow not available; will skip TF model unless installed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0511f8d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scan: D:\\Code\\Arduino\\ESP32\\ESP32_MPU6050_Air_Mouse\\desktop_qt\\ml_logs -> 2 files\n",
      "Scan: D:\\Code\\Arduino\\ESP32\\ESP32_MPU6050_Air_Mouse\\desktop_app\\ml_logs -> 0 files\n",
      "Scan: D:\\Code\\Arduino\\ESP32\\ESP32_MPU6050_Air_Mouse\\ml_logs -> 0 files\n",
      "Total discovered CSV files: 2\n",
      "Files loaded: ['flick_left_20251109_031457_8d5c3d8d_1.csv', 'flick_right_20251109_032720_3e1a6a5a_1.csv']\n",
      "[Info] Using 'timestamp_ms' as timestamp for flick_left_20251109_031457_8d5c3d8d_1.csv\n",
      "[Info] Using 'timestamp_ms' as timestamp for flick_right_20251109_032720_3e1a6a5a_1.csv\n",
      "Loaded rows: 608\n",
      "Columns: ['timestamp', 'ax', 'ay', 'az', 'gx', 'gy', 'gz', 'label', 'segment_id', 'sample_index', 'source_file']\n",
      "Per-label counts:\n",
      " label\n",
      "flick_right    433\n",
      "flick_left     175\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import os, glob, math, json, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy.signal import welch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Robust data directory discovery (tries multiple common locations)\n",
    "CANDIDATE_DIRS = [\n",
    "    Path(\"..\") / \"desktop_qt\" / \"ml_logs\",\n",
    "    Path(\"..\") / \"desktop_app\" / \"ml_logs\",\n",
    "    Path(\"..\") / \"ml_logs\",\n",
    "]\n",
    "found = []\n",
    "for d in CANDIDATE_DIRS:\n",
    "    files = sorted(glob.glob(str(d / \"*.csv\")))\n",
    "    print(f\"Scan: {d.resolve()} -> {len(files)} files\")\n",
    "    found.extend(files)\n",
    "\n",
    "# Default: use ALL discovered CSVs\n",
    "PIN_FILENAME = None  # set to a single filename (e.g., 'flick_left_20251109_101010.csv') to load only that one\n",
    "\n",
    "if PIN_FILENAME:\n",
    "    candidates = [p for p in found if p.endswith(PIN_FILENAME)]\n",
    "    if not candidates:\n",
    "        raise FileNotFoundError(f\"Pinned CSV '{PIN_FILENAME}' not found in: {[str(p) for p in CANDIDATE_DIRS]}\")\n",
    "    all_files = candidates\n",
    "else:\n",
    "    all_files = found\n",
    "\n",
    "print(f\"Total discovered CSV files: {len(all_files)}\")\n",
    "assert all_files, \"No CSV log files found in any ml_logs directory. Check the path or move/symlink your logs.\"\n",
    "print(\"Files loaded:\", [os.path.basename(p) for p in all_files])\n",
    "\n",
    "frames = []\n",
    "required_cols = {\"timestamp\",\"ax\",\"ay\",\"az\",\"gx\",\"gy\",\"gz\",\"label\"}\n",
    "alt_time_cols = [\"time\",\"ts\",\"millis\",\"timestamp_ms\",\"time_ms\",\"t_ms\"]\n",
    "for f in all_files:\n",
    "    # Robust CSV read with fallback delimiter sniffing\n",
    "    try:\n",
    "        df = pd.read_csv(f)\n",
    "    except Exception:\n",
    "        df = pd.read_csv(f, sep=None, engine=\"python\")\n",
    "    cols = set(df.columns)\n",
    "    # Timestamp fallback\n",
    "    if \"timestamp\" not in cols:\n",
    "        alt = next((c for c in alt_time_cols if c in cols), None)\n",
    "        if alt:\n",
    "            print(f\"[Info] Using '{alt}' as timestamp for {os.path.basename(f)}\")\n",
    "            df.rename(columns={alt:\"timestamp\"}, inplace=True)\n",
    "    # Coerce numeric timestamp\n",
    "    if \"timestamp\" in df.columns:\n",
    "        df[\"timestamp\"] = pd.to_numeric(df[\"timestamp\"], errors=\"coerce\")\n",
    "    # Label fallback from filename when missing\n",
    "    if \"label\" not in df.columns:\n",
    "        inferred_label = Path(f).stem.split(\"_\")[0]\n",
    "        print(f\"[Info] No 'label' column in {os.path.basename(f)} -> using '{inferred_label}' from filename\")\n",
    "        df[\"label\"] = inferred_label\n",
    "    # Required sensor columns coercion\n",
    "    for c in [\"ax\",\"ay\",\"az\",\"gx\",\"gy\",\"gz\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "        else:\n",
    "            raise ValueError(f\"Missing required column '{c}' in {f}\")\n",
    "    before = len(df)\n",
    "    df = df.dropna(subset=[\"timestamp\",\"ax\",\"ay\",\"az\",\"gx\",\"gy\",\"gz\",\"label\"]).copy()\n",
    "    dropped = before - len(df)\n",
    "    if dropped:\n",
    "        print(f\"[Info] Dropped {dropped} rows with NaNs in required fields from {os.path.basename(f)}\")\n",
    "    df['source_file'] = os.path.basename(f)\n",
    "    frames.append(df)\n",
    "\n",
    "raw = pd.concat(frames, ignore_index=True)\n",
    "print(\"Loaded rows:\", len(raw))\n",
    "print(\"Columns:\", list(raw.columns))\n",
    "print(\"Per-label counts:\\n\", raw['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4444fa22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CSV diagnostics ===\n",
      "Candidate directories:\n",
      " - D:\\Code\\Arduino\\ESP32\\ESP32_MPU6050_Air_Mouse\\desktop_qt\\ml_logs exists= True\n",
      " - D:\\Code\\Arduino\\ESP32\\ESP32_MPU6050_Air_Mouse\\desktop_app\\ml_logs exists= False\n",
      " - D:\\Code\\Arduino\\ESP32\\ESP32_MPU6050_Air_Mouse\\ml_logs exists= False\n",
      "Inspecting: ..\\desktop_qt\\ml_logs\\flick_left_20251109_031457_8d5c3d8d_1.csv\n",
      "Shape: (175, 10)\n",
      "Columns: ['timestamp_ms', 'ax', 'ay', 'az', 'gx', 'gy', 'gz', 'label', 'segment_id', 'sample_index']\n",
      "Dtypes:\n",
      " timestamp_ms     int64\n",
      "ax               int64\n",
      "ay               int64\n",
      "az               int64\n",
      "gx               int64\n",
      "gy               int64\n",
      "gz               int64\n",
      "label           object\n",
      "segment_id      object\n",
      "sample_index     int64\n",
      "dtype: object\n",
      "Head:\n",
      "     timestamp_ms  ax  ay    az  gx  gy  gz       label segment_id  \\\n",
      "0  1762636495403 -58  -5  1018  28   4  10  flick_left   8d5c3d8d   \n",
      "1  1762636495428 -64   0  1018  14  10  -3  flick_left   8d5c3d8d   \n",
      "2  1762636495433 -55  -2  1018   8 -15  34  flick_left   8d5c3d8d   \n",
      "3  1762636495530 -64   0  1018  14  10  -3  flick_left   8d5c3d8d   \n",
      "4  1762636495530 -55  -2  1018   8 -15  34  flick_left   8d5c3d8d   \n",
      "\n",
      "   sample_index  \n",
      "0             0  \n",
      "1             1  \n",
      "2             2  \n",
      "3             3  \n",
      "4             4  \n",
      "Labels unique: ['flick_left']\n",
      "No timestamp-like column found (timestamp/time/ts/millis)\n"
     ]
    }
   ],
   "source": [
    "# CSV diagnostics (run after the data loading cell)\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"\\n=== CSV diagnostics ===\")\n",
    "print(\"Candidate directories:\")\n",
    "for d in CANDIDATE_DIRS:\n",
    "    try:\n",
    "        print(\" -\", d.resolve(), \"exists=\", d.exists())\n",
    "    except Exception:\n",
    "        print(\" -\", d, \"(resolve failed)\")\n",
    "\n",
    "# Pick a target file: PIN_FILENAME if set, else first available\n",
    "if PIN_FILENAME:\n",
    "    targets = [p for p in found if p.endswith(PIN_FILENAME)]\n",
    "else:\n",
    "    targets = found[:1]\n",
    "\n",
    "if not targets:\n",
    "    print(\"No CSVs found to inspect.\")\n",
    "else:\n",
    "    p = Path(targets[0])\n",
    "    print(\"Inspecting:\", p)\n",
    "    try:\n",
    "        df0 = pd.read_csv(p)\n",
    "        print(\"Shape:\", df0.shape)\n",
    "        print(\"Columns:\", list(df0.columns))\n",
    "        print(\"Dtypes:\\n\", df0.dtypes)\n",
    "        print(\"Head:\\n\", df0.head())\n",
    "        if 'label' in df0.columns:\n",
    "            print(\"Labels unique:\", df0['label'].unique())\n",
    "        # Try to interpret time\n",
    "        ts_col = 'timestamp' if 'timestamp' in df0.columns else (next((c for c in ['time','ts','millis'] if c in df0.columns), None))\n",
    "        if ts_col:\n",
    "            ts_num = pd.to_numeric(df0[ts_col], errors='coerce')\n",
    "            valid = ts_num.dropna()\n",
    "            if len(valid) > 1:\n",
    "                deltas = np.diff(np.sort(valid.to_numpy()))\n",
    "                if len(deltas):\n",
    "                    print(\"Median dt:\", np.median(deltas), \"Max dt:\", np.max(deltas))\n",
    "            print(\"Timestamp sample:\", df0[ts_col].head().tolist())\n",
    "        else:\n",
    "            print(\"No timestamp-like column found (timestamp/time/ts/millis)\")\n",
    "    except Exception as e:\n",
    "        print(\"Error reading CSV:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "04005ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferred/used sampling interval median_dt=5.000; effective_hz≈0.20\n",
      "Adjusting EXPECTED_HZ from 100 to inferred 0.20\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Window length too short (0 samples). Increase WINDOW_SECONDS or collect higher-rate data.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 42\u001b[0m\n\u001b[0;32m     40\u001b[0m SAMPLES_PER_WINDOW \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(WINDOW_SECONDS \u001b[38;5;241m*\u001b[39m EXPECTED_HZ)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m SAMPLES_PER_WINDOW \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m5\u001b[39m:\n\u001b[1;32m---> 42\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWindow length too short (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSAMPLES_PER_WINDOW\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m samples). Increase WINDOW_SECONDS or collect higher-rate data.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     43\u001b[0m STEP \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mint\u001b[39m(SAMPLES_PER_WINDOW \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m WINDOW_OVERLAP)))\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSamples/window: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSAMPLES_PER_WINDOW\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, step: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSTEP\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Window length too short (0 samples). Increase WINDOW_SECONDS or collect higher-rate data."
     ]
    }
   ],
   "source": [
    "# Windowing helper with adaptive sampling and diagnostics\n",
    "# Determine effective sampling rate from timestamps\n",
    "if raw.empty:\n",
    "    raise ValueError(\"Raw dataframe is empty – no data loaded. Check DATA_DIR and CSV logs.\")\n",
    "\n",
    "# Infer time unit (ms vs s) using median delta; fallback if insufficient data\n",
    "sample_deltas = []\n",
    "for lbl, grp in raw.groupby('label'):\n",
    "    ts_arr = grp['timestamp'].to_numpy()\n",
    "    if len(ts_arr) > 1:\n",
    "        d = np.diff(np.sort(ts_arr))\n",
    "        # Filter obviously bad (negative) deltas\n",
    "        d = d[d > 0]\n",
    "        if len(d):\n",
    "            sample_deltas.extend(d.tolist())\n",
    "\n",
    "if not sample_deltas:\n",
    "    # Fallback: assume user intended EXPECTED_HZ; fabricate median_dt accordingly\n",
    "    print(f\"[Fallback] Not enough timestamp gaps to infer sampling rate (need >=2 samples in at least one label). Using EXPECTED_HZ={EXPECTED_HZ}.\")\n",
    "    # Assume milliseconds scale if EXPECTED_HZ > 5\n",
    "    if EXPECTED_HZ > 5:\n",
    "        median_dt = 1000.0 / EXPECTED_HZ  # pretend ms intervals\n",
    "        effective_hz = EXPECTED_HZ\n",
    "    else:\n",
    "        median_dt = 1.0 / EXPECTED_HZ\n",
    "        effective_hz = EXPECTED_HZ\n",
    "else:\n",
    "    median_dt = np.median(sample_deltas)\n",
    "    if median_dt > 5:  # treat as ms\n",
    "        effective_hz = 1000.0 / median_dt\n",
    "    else:  # seconds\n",
    "        effective_hz = 1.0 / median_dt\n",
    "print(f\"Inferred/used sampling interval median_dt={median_dt:.3f}; effective_hz≈{effective_hz:.2f}\")\n",
    "\n",
    "# Allow override of EXPECTED_HZ if large deviation (>20%) and we had real inference\n",
    "if sample_deltas and abs(effective_hz - EXPECTED_HZ) / max(EXPECTED_HZ,1e-6) > 0.2:\n",
    "    print(f\"Adjusting EXPECTED_HZ from {EXPECTED_HZ} to inferred {effective_hz:.2f}\")\n",
    "    EXPECTED_HZ = effective_hz\n",
    "\n",
    "SAMPLES_PER_WINDOW = int(WINDOW_SECONDS * EXPECTED_HZ)\n",
    "if SAMPLES_PER_WINDOW < 5:\n",
    "    raise ValueError(f\"Window length too short ({SAMPLES_PER_WINDOW} samples). Increase WINDOW_SECONDS or collect higher-rate data.\")\n",
    "STEP = max(1, int(SAMPLES_PER_WINDOW * (1 - WINDOW_OVERLAP)))\n",
    "print(f\"Samples/window: {SAMPLES_PER_WINDOW}, step: {STEP}\")\n",
    "\n",
    "# Diagnostic: per-label sample counts\n",
    "label_counts = raw['label'].value_counts().to_dict()\n",
    "print(\"Per-label sample counts:\", label_counts)\n",
    "min_label_samples = min(label_counts.values())\n",
    "if min_label_samples < SAMPLES_PER_WINDOW:\n",
    "    print(f\"WARNING: Smallest label has only {min_label_samples} samples (< {SAMPLES_PER_WINDOW}).\")\n",
    "    if min_label_samples >= 3:\n",
    "        adjusted_samples = max(3, int(min_label_samples * 0.9))\n",
    "        if adjusted_samples < SAMPLES_PER_WINDOW:\n",
    "            print(f\"Auto-adjusting SAMPLES_PER_WINDOW from {SAMPLES_PER_WINDOW} to {adjusted_samples}.\")\n",
    "            SAMPLES_PER_WINDOW = adjusted_samples\n",
    "            WINDOW_SECONDS = SAMPLES_PER_WINDOW / EXPECTED_HZ\n",
    "            STEP = max(1, int(SAMPLES_PER_WINDOW * (1 - WINDOW_OVERLAP)))\n",
    "            print(f\"New window config: WINDOW_SECONDS={WINDOW_SECONDS:.3f}, samples={SAMPLES_PER_WINDOW}, step={STEP}\")\n",
    "    else:\n",
    "        print(\"Too few samples (<3) for any window; collect more data.\")\n",
    "\n",
    "# Force a smaller window if needed for tiny logs (simple guard)\n",
    "if SAMPLES_PER_WINDOW > 20:\n",
    "    SAMPLES_PER_WINDOW = max(5, min(SAMPLES_PER_WINDOW, 20))\n",
    "    WINDOW_SECONDS = SAMPLES_PER_WINDOW / EXPECTED_HZ\n",
    "    STEP = max(1, int(SAMPLES_PER_WINDOW * (1 - WINDOW_OVERLAP)))\n",
    "    print(f\"[Guard] Using smaller window: WINDOW_SECONDS={WINDOW_SECONDS:.3f}, samples={SAMPLES_PER_WINDOW}, step={STEP}\")\n",
    "\n",
    "ANOMALY_MULT = 5.0  # relaxed anomaly filter\n",
    "\n",
    "windows = []\n",
    "labels = []\n",
    "\n",
    "# If chronological fallback needed (e.g., single label), enable when only one label present\n",
    "chronological_mode = (len(label_counts) == 1)\n",
    "if chronological_mode:\n",
    "    print(\"[Mode] Single label detected; using chronological sliding windows.\")\n",
    "    g = raw.sort_values('timestamp')\n",
    "    arr = g[['ax','ay','az','gx','gy','gz']].to_numpy()\n",
    "    ts = g['timestamp'].to_numpy()\n",
    "    if len(arr) >= SAMPLES_PER_WINDOW:\n",
    "        for start in range(0, len(arr) - SAMPLES_PER_WINDOW + 1, STEP):\n",
    "            segment = arr[start:start+SAMPLES_PER_WINDOW]\n",
    "            tseg = ts[start:start+SAMPLES_PER_WINDOW]\n",
    "            deltas = np.diff(tseg)\n",
    "            if len(deltas) and np.max(deltas) > ANOMALY_MULT * max(np.median(deltas),1e-6):\n",
    "                continue\n",
    "            windows.append(segment)\n",
    "            labels.append(g['label'].iloc[start])\n",
    "else:\n",
    "    for label, group in raw.groupby('label'):\n",
    "        g = group.sort_values('timestamp')\n",
    "        arr = g[['ax','ay','az','gx','gy','gz']].to_numpy()\n",
    "        ts = g['timestamp'].to_numpy()\n",
    "        if len(arr) < SAMPLES_PER_WINDOW:\n",
    "            continue\n",
    "        for start in range(0, len(arr) - SAMPLES_PER_WINDOW + 1, STEP):\n",
    "            segment = arr[start:start+SAMPLES_PER_WINDOW]\n",
    "            tseg = ts[start:start+SAMPLES_PER_WINDOW]\n",
    "            deltas = np.diff(tseg)\n",
    "            if len(deltas) and np.max(deltas) > ANOMALY_MULT * max(np.median(deltas),1e-6):\n",
    "                continue\n",
    "            windows.append(segment)\n",
    "            labels.append(label)\n",
    "\n",
    "print(f\"Created {len(windows)} windows total\")\n",
    "unique_labels = sorted(set(labels))\n",
    "if not windows:\n",
    "    print(\"No windows created. Collect more samples OR reduce WINDOW_SECONDS to <0.3 OR increase overlap.\")\n",
    "    # Provide empty arrays for downstream graceful handling\n",
    "    y = np.array([])\n",
    "    X = np.empty((0, SAMPLES_PER_WINDOW, 6))\n",
    "else:\n",
    "    label_to_idx = {l:i for i,l in enumerate(unique_labels)}\n",
    "    y = np.array([label_to_idx[l] for l in labels])\n",
    "    X = np.array(windows)\n",
    "    print(\"Window tensor shape:\", X.shape)\n",
    "    print(\"Labels:\", unique_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66126ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (0,)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 51\u001b[0m\n\u001b[0;32m     49\u001b[0m X_feats \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(FEATURES)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature matrix shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, X_feats\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 51\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFirst row sample:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mX_feats\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m[:\u001b[38;5;241m10\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal features:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(feature_names))\n",
      "\u001b[1;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "# Feature extraction\n",
    "# We'll compute per-axis statistics + overall magnitude stats + simple band energy via Welch.\n",
    "\n",
    "FEATURES = []\n",
    "feature_names = []\n",
    "\n",
    "if X.shape[0] == 0:\n",
    "    raise RuntimeError(\"No windows available for feature extraction – revisit windowing parameters or collect more data.\")\n",
    "\n",
    "for i in range(X.shape[0]):\n",
    "    segment = X[i]  # (samples, 6)\n",
    "    ax, ay, az, gx, gy, gz = segment.T\n",
    "    feats = []\n",
    "    def add_stats(prefix, data):\n",
    "        feats.extend([\n",
    "            np.mean(data),\n",
    "            np.std(data),\n",
    "            np.min(data),\n",
    "            np.max(data),\n",
    "            np.median(data),\n",
    "            np.percentile(data, 25),\n",
    "            np.percentile(data, 75),\n",
    "        ])\n",
    "        if i == 0:\n",
    "            feature_names.extend([\n",
    "                f\"{prefix}_mean\", f\"{prefix}_std\", f\"{prefix}_min\", f\"{prefix}_max\",\n",
    "                f\"{prefix}_median\", f\"{prefix}_p25\", f\"{prefix}_p75\"\n",
    "            ])\n",
    "\n",
    "    for prefix, data in [(\"ax\", ax),(\"ay\", ay),(\"az\", az),(\"gx\", gx),(\"gy\", gy),(\"gz\", gz)]:\n",
    "        add_stats(prefix, data)\n",
    "\n",
    "    # Magnitudes\n",
    "    a_mag = np.sqrt(ax*ax + ay*ay + az*az)\n",
    "    g_mag = np.sqrt(gx*gx + gy*gy + gz*gz)\n",
    "    add_stats(\"a_mag\", a_mag)\n",
    "    add_stats(\"g_mag\", g_mag)\n",
    "\n",
    "    # Simple frequency band energies for accel magnitude\n",
    "    freqs, psd = welch(a_mag, fs=EXPECTED_HZ, nperseg=min(len(a_mag), 128))\n",
    "    bands = [(0.5,2),(2,5),(5,10),(10,20)]\n",
    "    for (lo, hi) in bands:\n",
    "        mask = (freqs >= lo) & (freqs < hi)\n",
    "        energy = np.sum(psd[mask])\n",
    "        feats.append(energy)\n",
    "        if i == 0:\n",
    "            feature_names.append(f\"a_mag_energy_{lo}_{hi}\")\n",
    "\n",
    "    FEATURES.append(feats)\n",
    "\n",
    "X_feats = np.array(FEATURES)\n",
    "print(\"Feature matrix shape:\", X_feats.shape)\n",
    "if X_feats.shape[0] > 0:\n",
    "    print(\"First row sample:\", X_feats[0][:10], \"...\")\n",
    "print(\"Total features:\", len(feature_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a3fb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train / test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_feats, y, test_size=0.25, random_state=RANDOM_STATE, stratify=y)\n",
    "print(\"Train size:\", X_train.shape, \"Test size:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4eea970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline model: Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=200, max_depth=None, class_weight=\"balanced\", random_state=RANDOM_STATE)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=unique_labels))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion matrix:\\n\", cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a688e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "def plot_confusion_matrix(cm, labels):\n",
    "    fig, ax = plt.subplots(figsize=(5,5))\n",
    "    im = ax.imshow(cm, cmap='Blues')\n",
    "    ax.set_xticks(range(len(labels)))\n",
    "    ax.set_yticks(range(len(labels)))\n",
    "    ax.set_xticklabels(labels, rotation=45, ha='right')\n",
    "    ax.set_yticklabels(labels)\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, cm[i,j], ha='center', va='center', color='black')\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('True')\n",
    "    ax.set_title('Confusion Matrix - RF')\n",
    "    fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_confusion_matrix(confusion_matrix(y_test, y_pred), unique_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671a445c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: TensorFlow small dense model\n",
    "USE_TF = 'tensorflow' in sys.modules\n",
    "if USE_TF:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras import layers, models\n",
    "    tf.random.set_seed(RANDOM_STATE)\n",
    "    input_dim = X_feats.shape[1]\n",
    "    num_classes = len(unique_labels)\n",
    "\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(input_dim,)),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    history = model.fit(X_train, y_train, epochs=25, batch_size=32, validation_split=0.2, verbose=0)\n",
    "    tf_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "    print(classification_report(y_test, tf_pred, target_names=unique_labels))\n",
    "else:\n",
    "    print(\"TensorFlow not installed; skipping TF model.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19638119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFLite int8 export (only if TF model trained)\n",
    "if USE_TF:\n",
    "    import tensorflow as tf\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "    # Representative dataset for full int8 quantization\n",
    "    def representative_dataset_generator():\n",
    "        for i in range(min(200, X_train.shape[0])):\n",
    "            yield [X_train[i:i+1].astype(np.float32)]\n",
    "    converter.representative_dataset = representative_dataset_generator\n",
    "\n",
    "    # Force int8 quantization for ops and I/O\n",
    "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "    converter.inference_input_type = tf.int8\n",
    "    converter.inference_output_type = tf.int8\n",
    "\n",
    "    tflite_model = converter.convert()\n",
    "    tflite_path = Path(\"gesture_model_int8.tflite\")\n",
    "    tflite_path.write_bytes(tflite_model)\n",
    "    print(f\"Saved TFLite int8 model: {tflite_path.resolve()}\")\n",
    "else:\n",
    "    print(\"Skipping TFLite export (TensorFlow not available).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbad77a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export model artifacts for firmware integration (labels + feature meta + optional C array)\n",
    "meta = {\n",
    "    \"labels\": unique_labels,\n",
    "    \"feature_names\": feature_names,\n",
    "    \"window_samples\": SAMPLES_PER_WINDOW,\n",
    "    \"overlap\": WINDOW_OVERLAP,\n",
    "    \"sampling_hz\": EXPECTED_HZ\n",
    "}\n",
    "with open(\"gesture_meta.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(meta, f, indent=2)\n",
    "print(\"Wrote gesture_meta.json\")\n",
    "\n",
    "# Optional: convert TFLite file into a C array for embedding in firmware\n",
    "TFLITE_FILENAME = \"gesture_model_int8.tflite\"\n",
    "if Path(TFLITE_FILENAME).exists():\n",
    "    data = Path(TFLITE_FILENAME).read_bytes()\n",
    "    array_name = \"gesture_model_int8\"\n",
    "    header_path = Path(\"gesture_model_data.h\")\n",
    "    with open(header_path, \"w\", encoding=\"utf-8\") as h:\n",
    "        h.write(\"#pragma once\\n#include <cstdint>\\n\\n\")\n",
    "        h.write(f\"// Auto-generated from {TFLITE_FILENAME} ({len(data)} bytes)\\n\")\n",
    "        h.write(f\"const unsigned int {array_name}_len = {len(data)};\\n\")\n",
    "        h.write(f\"const unsigned char {array_name}[] = {{\\n    \")\n",
    "        for i, b in enumerate(data):\n",
    "            h.write(str(b)+', ')\n",
    "            if (i+1) % 16 == 0:\n",
    "                h.write(\"\\n    \")\n",
    "        h.write(\"\\n};\\n\")\n",
    "    print(f\"Wrote {header_path.resolve()}\")\n",
    "else:\n",
    "    print(\"No TFLite model file present; skip C array generation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd6f8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Firmware integration notes (Markdown)\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(Markdown(r\"\"\"\n",
    "## Firmware Integration Steps\n",
    "1. Copy `gesture_model_data.h` into `lib/gesture_model/` (create folder) in PlatformIO project.\n",
    "2. Include the header where you run inference: `#include \"gesture_model_data.h\"`.\n",
    "3. Use TensorFlow Lite for Microcontrollers (add library) and set up a MicroMutableOpResolver with only needed ops (DEPTHWISE_CONV_2D not required for dense-only model).\n",
    "4. Mirror the feature extraction on-device:\n",
    "   - Maintain a ring buffer of the last `window_samples` raw readings (ax..gz).\n",
    "   - Every `STEP` samples, compute the same stats and band energies.\n",
    "5. Normalize or scale features if you added any normalization during training (none used here beyond raw units).\n",
    "6. Run inference; map output index to label via `gesture_meta.json` labels order.\n",
    "7. Threshold low-confidence predictions; optionally require stability over 2–3 consecutive windows.\n",
    "\n",
    "### Performance Considerations\n",
    "- RandomForest is NOT embedded; use TF-lite model for MCU.\n",
    "- Aim for < 30KB flash and < 10KB RAM for model + arena.\n",
    "- Reduce dense layer sizes if memory is tight (e.g., 32 -> 16 neurons).\n",
    "\n",
    "### Next Improvements\n",
    "- Add axis-specific filtering (e.g., 5 Hz low-pass) before feature extraction.\n",
    "- Evaluate additional features: skewness, kurtosis, gyro band energies.\n",
    "- Add gesture rejection class (\"none\") to reduce false positives.\n",
    "\"\"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4d7bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw CSV peek (first 20 lines of the chosen file)\n",
    "peek_file = None\n",
    "if PIN_FILENAME:\n",
    "    for p in found:\n",
    "        if p.endswith(PIN_FILENAME):\n",
    "            peek_file = p\n",
    "            break\n",
    "else:\n",
    "    peek_file = found[0] if found else None\n",
    "print(\"Peek file:\", peek_file)\n",
    "if peek_file:\n",
    "    try:\n",
    "        with open(peek_file, 'r', encoding='utf-8', errors='replace') as fh:\n",
    "            for i, line in enumerate(fh):\n",
    "                if i >= 20:\n",
    "                    break\n",
    "                print(f\"{i+1:02d}: {line.rstrip()}\")\n",
    "    except Exception as e:\n",
    "        print(\"Error peeking file:\", e)\n",
    "else:\n",
    "    print(\"No file found to peek.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
